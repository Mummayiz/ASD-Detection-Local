from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel, validator
from typing import List, Dict, Optional, Any
import joblib
import numpy as np
import pandas as pd
import os
import motor.motor_asyncio
from datetime import datetime
import json
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import base64
import cv2

# --- LOCAL_MODELS_PATH_PATCH START ---
import os
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
MODEL_DIR = os.environ.get('MODEL_DIR', os.path.join(BASE_DIR, 'models'))
# Wrap joblib.load so '/app/models/xxx' -> local MODEL_DIR/xxx when running locally
_original_joblib_load = joblib.load
def _joblib_load_wrapper(path, *args, **kwargs):
    if isinstance(path, str) and path.startswith('/app/models/'):
        path = os.path.join(MODEL_DIR, path.replace('/app/models/', ''))
    return _original_joblib_load(path, *args, **kwargs)
joblib.load = _joblib_load_wrapper
# --- LOCAL_MODELS_PATH_PATCH END ---
import asyncio
import logging
from bson import ObjectId
import random

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def clean_mongo_doc(doc):
    """Clean MongoDB document by removing ObjectId and converting datetime"""
    if doc is None:
        return None
    
    cleaned = {}
    for key, value in doc.items():
        if key == '_id':
            continue  # Skip ObjectId
        elif isinstance(value, ObjectId):
            cleaned[key] = str(value)
        elif isinstance(value, datetime):
            cleaned[key] = value.isoformat()
        elif isinstance(value, dict):
            cleaned[key] = clean_mongo_doc(value)
        elif isinstance(value, list):
            cleaned[key] = [clean_mongo_doc(item) if isinstance(item, dict) else item for item in value]
        else:
            cleaned[key] = value
    return cleaned

def json_encoder(obj):
    """Custom JSON encoder for MongoDB ObjectId and datetime objects"""
    if isinstance(obj, ObjectId):
        return str(obj)
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

app = FastAPI(
    title="ASD Detection API",
    description="Machine Learning API for Autism Spectrum Disorder Detection with Multi-Stage Assessment and Enhanced Eye Tracking Visualization",
    version="1.1.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files (React build)
app.mount("/static", StaticFiles(directory="frontend/build/static"), name="static")

@app.on_event("startup")
async def startup_event():
    """Load models on startup"""
    try:
        await load_models()
        logger.info("✅ Models loaded successfully on startup")
    except Exception as e:
        logger.error(f"❌ Failed to load models on startup: {str(e)}")
        # Don't crash the server if models fail to load
        # The server will still work with mock responses

# Database connection
MONGO_URL = os.environ.get('MONGO_URL', 'mongodb://localhost:27017')
client = motor.motor_asyncio.AsyncIOMotorClient(MONGO_URL)
db = client.asd_detection

# Global variables for models
models = {}
scalers = {}
encoders = {}

class PSO:
    """Particle Swarm Optimization for feature selection and model optimization"""
    
    def __init__(self, n_particles=20, n_iterations=50, w=0.5, c1=1.5, c2=1.5):
        self.n_particles = n_particles
        self.n_iterations = n_iterations
        self.w = w  # inertia weight
        self.c1 = c1  # cognitive parameter
        self.c2 = c2  # social parameter
        
    def optimize_features(self, X, y, model_type='rf'):
        """Optimize feature selection using PSO"""
        n_features = X.shape[1]
        
        # Initialize particles (binary encoding for feature selection)
        particles = np.random.randint(0, 2, (self.n_particles, n_features))
        velocities = np.random.uniform(-1, 1, (self.n_particles, n_features))
        
        # Track best positions
        personal_best = particles.copy()
        personal_best_scores = np.full(self.n_particles, -np.inf)
        global_best = particles[0].copy()
        global_best_score = -np.inf
        
        for iteration in range(self.n_iterations):
            for i in range(self.n_particles):
                # Evaluate fitness (model accuracy with selected features)
                selected_features = particles[i] == 1
                if np.sum(selected_features) == 0:  # At least one feature must be selected
                    selected_features[0] = True
                
                score = self._evaluate_features(X[:, selected_features], y, model_type)
                
                # Update personal best
                if score > personal_best_scores[i]:
                    personal_best_scores[i] = score
                    personal_best[i] = particles[i].copy()
                
                # Update global best
                if score > global_best_score:
                    global_best_score = score
                    global_best = particles[i].copy()
            
            # Update velocities and positions
            for i in range(self.n_particles):
                r1, r2 = np.random.random(n_features), np.random.random(n_features)
                
                velocities[i] = (self.w * velocities[i] + 
                               self.c1 * r1 * (personal_best[i] - particles[i]) + 
                               self.c2 * r2 * (global_best - particles[i]))
                
                # Update positions using sigmoid function for binary encoding
                sigmoid_v = 1 / (1 + np.exp(-velocities[i]))
                particles[i] = (np.random.random(n_features) < sigmoid_v).astype(int)
        
        return global_best, global_best_score
    
    def _evaluate_features(self, X, y, model_type):
        """Evaluate feature subset using cross-validation"""
        if X.shape[1] == 0:
            return 0
            
        try:
            if model_type == 'rf':
                model = RandomForestClassifier(n_estimators=10, random_state=42)
            else:
                model = SVC(random_state=42)
            
            # Simple train-test split for speed
            split_idx = int(0.8 * len(X))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            if len(np.unique(y_train)) < 2:  # Not enough classes
                return 0
                
            model.fit(X_train, y_train)
            score = model.score(X_test, y_test)
            return score
        except:
            return 0

    def optimize_prediction(self, predictions, weights=None):
        """Optimize final prediction using PSO ensemble weighting"""
        if weights is None:
            weights = np.random.random(len(predictions))
            weights = weights / np.sum(weights)
        
        # Use PSO to find optimal weights for ensemble
        n_models = len(predictions)
        particles = np.random.random((self.n_particles, n_models))
        # Normalize weights
        particles = particles / particles.sum(axis=1, keepdims=True)
        
        velocities = np.random.uniform(-0.1, 0.1, (self.n_particles, n_models))
        
        personal_best = particles.copy()
        personal_best_scores = np.full(self.n_particles, -np.inf)
        global_best = particles[0].copy()
        global_best_score = -np.inf
        
        for iteration in range(min(self.n_iterations, 20)):  # Fewer iterations for speed
            for i in range(self.n_particles):
                # Calculate weighted ensemble prediction
                ensemble_pred = np.average(predictions, weights=particles[i])
                
                # Simple fitness: diversity + accuracy proxy
                score = self._ensemble_fitness(predictions, particles[i], ensemble_pred)
                
                if score > personal_best_scores[i]:
                    personal_best_scores[i] = score
                    personal_best[i] = particles[i].copy()
                
                if score > global_best_score:
                    global_best_score = score
                    global_best = particles[i].copy()
            
            # Update particles
            for i in range(self.n_particles):
                r1, r2 = np.random.random(n_models), np.random.random(n_models)
                
                velocities[i] = (self.w * velocities[i] + 
                               self.c1 * r1 * (personal_best[i] - particles[i]) + 
                               self.c2 * r2 * (global_best - particles[i]))
                
                particles[i] += velocities[i]
                # Normalize weights
                particles[i] = np.abs(particles[i])
                particles[i] = particles[i] / np.sum(particles[i])
        
        return global_best, global_best_score
    
    def _ensemble_fitness(self, predictions, weights, ensemble_pred):
        """Calculate fitness for ensemble weights"""
        # Combine accuracy proxy with diversity
        diversity = np.std([pred * weight for pred, weight in zip(predictions, weights)])
        confidence = abs(ensemble_pred - 0.5)  # Distance from uncertainty
        return confidence + 0.1 * diversity

class BehavioralAssessment(BaseModel):
    """Behavioral questionnaire data"""
    A1_Score: float  # Social responsiveness - now supports 0, 0.5, 1
    A2_Score: float  # Communication patterns
    A3_Score: float  # Repetitive behaviors
    A4_Score: float  # Social interaction
    A5_Score: float  # Attention to detail
    A6_Score: float  # Sensory sensitivity
    A7_Score: float  # Language development
    A8_Score: float  # Motor skills
    A9_Score: float  # Behavioral flexibility
    A10_Score: float  # Emotional regulation
    age: float
    gender: str  # 'f' or 'm'
    
    @validator('A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 
              'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score')
    def validate_scores(cls, v):
        if v not in [0, 0.5, 1]:
            raise ValueError('Scores must be 0, 0.5, or 1')
        return v
    
    @validator('age')
    def validate_age(cls, v):
        if v < 0 or v > 100:
            raise ValueError('Age must be between 0 and 100')
        return v
    
    @validator('gender')
    def validate_gender(cls, v):
        if v not in ['f', 'm']:
            raise ValueError('Gender must be f or m')
        return v

class EyeTrackingData(BaseModel):
    """Eye tracking features from webcam analysis"""
    fixation_count: float
    mean_saccade: float
    max_saccade: float
    std_saccade: float
    mean_x: float
    mean_y: float
    std_x: float
    std_y: float
    mean_pupil: float

class FacialAnalysisData(BaseModel):
    """Facial analysis features"""
    facial_features: List[float]  # CNN features from facial analysis
    emotion_scores: Dict[str, float]
    attention_patterns: Dict[str, float]

class AssessmentResult(BaseModel):
    """Complete assessment result"""
    session_id: str
    behavioral_prediction: Optional[Dict[str, Any]] = None
    eye_tracking_prediction: Optional[Dict[str, Any]] = None
    facial_analysis_prediction: Optional[Dict[str, Any]] = None
    final_prediction: Dict[str, Any]
    confidence_score: float
    explanation: Dict[str, Any]
    timestamp: str

# @app.on_event("startup")
# async def load_models():
    """Load trained ML models on startup"""
    global models, scalers, encoders
    
    try:
        # Determine model path based on environment
        if os.path.exists('/app/models/') and not os.path.exists('models/'):
            model_path = '/app/models/'
        else:
            model_path = 'models/'
        logger.info(f"Loading models from: {model_path}")
        
        # Load behavioral models
        models['behavioral_rf'] = joblib.load(f'{model_path}behavioral_rf_model.joblib')
        models['behavioral_svm'] = joblib.load(f'{model_path}behavioral_svm_model.joblib')
        scalers['behavioral'] = joblib.load(f'{model_path}behavioral_scaler.joblib')
        encoders['behavioral'] = joblib.load(f'{model_path}behavioral_label_encoder.joblib')
        
        logger.info("Behavioral models loaded successfully")
        
        # Load eye tracking models if available
        if os.path.exists(f'{model_path}eye_tracking_rf_model.joblib'):
            models['eye_tracking_rf'] = joblib.load(f'{model_path}eye_tracking_rf_model.joblib')
            models['eye_tracking_svm'] = joblib.load(f'{model_path}eye_tracking_svm_model.joblib')
            scalers['eye_tracking'] = joblib.load(f'{model_path}eye_tracking_scaler.joblib')
            logger.info("Eye tracking models loaded successfully")
        
        logger.info("All models loaded successfully")
        
    except Exception as e:
        logger.error(f"Error loading models: {str(e)}")
        logger.warning("Continuing without models - using fallback assessments")

@app.get("/")
async def serve_frontend():
    """Serve the React frontend"""
    try:
        return FileResponse("frontend/build/index.html")
    except Exception as e:
        logger.error(f"Error serving frontend: {e}")
        return {"message": "ASD Detection API is running", "status": "ok"}

@app.get("/api/")
async def api_root():
    """API Root endpoint"""
    return {
        "message": "ASD Detection API",
        "version": "1.0.0",
        "status": "active",
        "stages": ["behavioral", "eye_tracking", "facial_analysis"]
    }

@app.get("/health")
async def health_check():
    """Health check endpoint - ultra simple"""
    return {"status": "ok"}

@app.get("/api/health")
async def api_health_check():
    """API Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "models_loaded": len(models),
        "available_stages": ["behavioral", "eye_tracking", "facial_analysis"]
    }

@app.get("/debug/models")
async def debug_models():
    """Debug endpoint to check if models are loaded"""
    model_path = '/app/models/' if os.path.exists('/app/models/') else 'models/'
    return {
        "server_version": "1.0.1",
        "models_loaded": list(models.keys()),
        "scalers_loaded": list(scalers.keys()),
        "encoders_loaded": list(encoders.keys()),
        "model_path": model_path,
        "models_exist": {
            "behavioral_rf": os.path.exists(f"{model_path}behavioral_rf_model.joblib"),
            "behavioral_svm": os.path.exists(f"{model_path}behavioral_svm_model.joblib"),
            "behavioral_scaler": os.path.exists(f"{model_path}behavioral_scaler.joblib"),
            "behavioral_encoder": os.path.exists(f"{model_path}behavioral_label_encoder.joblib"),
        }
    }

@app.post("/api/assessment/behavioral")
async def assess_behavioral(data: BehavioralAssessment):
    """Stage 1: Behavioral Assessment with PSO optimization"""
    try:
        # Check if models are loaded
        if 'behavioral_rf' not in models or 'behavioral_svm' not in models:
            logger.warning("Models not loaded, using fallback assessment")
            # Fallback to simple rule-based assessment
            total_score = sum([
                data.A1_Score, data.A2_Score, data.A3_Score, data.A4_Score, data.A5_Score,
                data.A6_Score, data.A7_Score, data.A8_Score, data.A9_Score, data.A10_Score
            ])
            
            # Simple threshold-based prediction
            prediction = 1 if total_score >= 6 else 0
            confidence = min(0.95, 0.5 + (total_score / 10) * 0.4)
            
            return {
                'prediction': prediction,
                'probability': confidence,
                'confidence': confidence,
                'model_results': {
                    'method': 'fallback_rule_based',
                    'total_score': total_score,
                    'threshold': 6
                },
                'explanation': {
                    'method': 'Simple rule-based assessment (models not loaded)',
                    'total_score': total_score,
                    'threshold_used': 6,
                    'reasoning': f'Total score {total_score} {"≥" if total_score >= 6 else "<"} 6 threshold'
                },
                'timestamp': datetime.now().isoformat()
            }
        
        # Prepare features
        features = np.array([[
            data.A1_Score, data.A2_Score, data.A3_Score, data.A4_Score, data.A5_Score,
            data.A6_Score, data.A7_Score, data.A8_Score, data.A9_Score, data.A10_Score,
            data.age, 1 if data.gender == 'm' else 0  # Encoded gender
        ]])
        
        # Scale features
        features_scaled = scalers['behavioral'].transform(features)
        
        # Make base predictions
        rf_pred = models['behavioral_rf'].predict_proba(features_scaled)[0]
        svm_pred = models['behavioral_svm'].predict_proba(features_scaled)[0]
        
        # Use PSO for optimal ensemble weighting
        pso = PSO(n_particles=15, n_iterations=30)
        base_predictions = [rf_pred[1], svm_pred[1]]  # Probability of ASD class
        
        optimal_weights, pso_score = pso.optimize_prediction(base_predictions)
        
        # PSO-optimized ensemble prediction
        pso_prob = np.average(base_predictions, weights=optimal_weights)
        pso_pred = 1 if pso_prob > 0.5 else 0
        
        # Feature importance analysis
        feature_importance = models['behavioral_rf'].feature_importances_
        feature_names = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                        'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age', 'gender']
        
        top_features = {}
        feature_values = features[0]
        for i, (name, importance) in enumerate(zip(feature_names, feature_importance)):
            if importance > 0.05:  # Only significant features
                top_features[name] = {
                    'importance': float(importance),
                    'value': float(feature_values[i]),
                    'contribution': float(importance * feature_values[i])
                }
        
        # Generate explanation
        explanation = generate_behavioral_explanation(pso_pred, pso_prob, top_features)
        
        result = {
            'prediction': int(pso_pred),
            'probability': float(pso_prob),
            'confidence': float(pso_score),
            'model_results': {
                'random_forest': {'probability': float(rf_pred[1]), 'prediction': int(rf_pred[1] > 0.5)},
                'svm': {'probability': float(svm_pred[1]), 'prediction': int(svm_pred[1] > 0.5)},
                'pso': {'probability': float(pso_prob), 'prediction': int(pso_pred), 'weights': optimal_weights.tolist()}
            },
            'explanation': explanation,
            'stage': 'behavioral',
            'timestamp': datetime.now().isoformat()
        }
        
        # Store result in database
        await db.assessments.insert_one({
            'stage': 'behavioral',
            'data': data.dict(),
            'result': result,
            'timestamp': datetime.now()
        })
        
        return result
        
    except Exception as e:
        logger.error(f"Behavioral assessment error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Assessment failed: {str(e)}")

@app.post("/api/assessment/eye_tracking")
async def assess_eye_tracking(data: EyeTrackingData):
    """Stage 2: Eye Tracking Assessment with PSO optimization"""
    try:
        if 'eye_tracking_rf' not in models:
            raise HTTPException(status_code=501, detail="Eye tracking models not available")
        
        # Prepare features
        features = np.array([[
            data.fixation_count, data.mean_saccade, data.max_saccade, data.std_saccade,
            data.mean_x, data.mean_y, data.std_x, data.std_y, data.mean_pupil
        ]])
        
        # Scale features
        features_scaled = scalers['eye_tracking'].transform(features)
        
        # Make base predictions
        rf_pred = models['eye_tracking_rf'].predict_proba(features_scaled)[0]
        svm_pred = models['eye_tracking_svm'].predict_proba(features_scaled)[0]
        
        # Use PSO for optimal ensemble weighting
        pso = PSO(n_particles=15, n_iterations=30)
        base_predictions = [rf_pred[1], svm_pred[1]]  # Probability of ASD class
        
        optimal_weights, pso_score = pso.optimize_prediction(base_predictions)
        
        # PSO-optimized ensemble prediction
        pso_prob = np.average(base_predictions, weights=optimal_weights)
        pso_pred = 1 if pso_prob > 0.5 else 0
        
        # Feature importance analysis
        feature_importance = models['eye_tracking_rf'].feature_importances_
        feature_names = ['fixation_count', 'mean_saccade', 'max_saccade', 'std_saccade',
                        'mean_x', 'mean_y', 'std_x', 'std_y', 'mean_pupil']
        
        top_features = {}
        feature_values = features[0]
        for i, (name, importance) in enumerate(zip(feature_names, feature_importance)):
            if importance > 0.05:
                top_features[name] = {
                    'importance': float(importance),
                    'value': float(feature_values[i]),
                    'description': get_eye_tracking_description(name)
                }
        
        # Generate explanation
        explanation = generate_eye_tracking_explanation(pso_pred, pso_prob, top_features)
        
        result = {
            'prediction': int(pso_pred),
            'probability': float(pso_prob),
            'confidence': float(pso_score),
            'model_results': {
                'random_forest': {'probability': float(rf_pred[1]), 'prediction': int(rf_pred[1] > 0.5)},
                'svm': {'probability': float(svm_pred[1]), 'prediction': int(svm_pred[1] > 0.5)},
                'pso': {'probability': float(pso_prob), 'prediction': int(pso_pred), 'weights': optimal_weights.tolist()},
                'visualization_enabled': True,
                'gaze_metrics': {
                    'fixation_count': float(data.fixation_count),
                    'mean_saccade': float(data.mean_saccade),
                    'blink_rate': float(data.blink_rate),
                    'gaze_stability': float(data.gaze_stability),
                    'avg_fixation_duration': float(data.avg_fixation_duration)
                }
            },
            'explanation': explanation,
            'stage': 'eye_tracking', 
            'timestamp': datetime.now().isoformat()
        }
        
        # Store result in database
        await db.assessments.insert_one({
            'stage': 'eye_tracking',
            'data': data.dict(),
            'result': result,
            'timestamp': datetime.now()
        })
        
        return result
        
    except Exception as e:
        logger.error(f"Eye tracking assessment error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Assessment failed: {str(e)}")

@app.post("/api/assessment/facial_analysis")
async def assess_facial_analysis(data: FacialAnalysisData):
    """Stage 3: Facial Analysis Assessment"""
    try:
        # For now, return a mock result since CNN training would need more setup
        # In a full implementation, this would use the trained CNN model
        
        # Simple analysis based on facial features
        feature_mean = np.mean(data.facial_features) if data.facial_features else 0.5
        
        # Mock prediction based on attention patterns and emotions
        attention_score = data.attention_patterns.get('attention_to_faces', 0.5)
        emotion_variability = np.std(list(data.emotion_scores.values())) if data.emotion_scores else 0.5
        
        # Combine features for prediction
        combined_score = (feature_mean * 0.4 + attention_score * 0.4 + emotion_variability * 0.2)
        prediction = 1 if combined_score > 0.6 else 0
        
        explanation = {
            'summary': f"Facial analysis {'indicates' if prediction else 'does not indicate'} ASD patterns",
            'key_factors': {
                'attention_to_faces': attention_score,
                'emotion_variability': emotion_variability,
                'facial_features_score': feature_mean
            },
            'interpretation': generate_facial_explanation(prediction, combined_score, data)
        }
        
        result = {
            'prediction': int(prediction),
            'probability': float(combined_score),
            'confidence': float(abs(combined_score - 0.5) * 2),
            'explanation': explanation,
            'stage': 'facial_analysis',
            'timestamp': datetime.now().isoformat()
        }
        
        # Store result in database
        await db.assessments.insert_one({
            'stage': 'facial_analysis',
            'data': data.dict(),
            'result': result,
            'timestamp': datetime.now()
        })
        
        return result
        
    except Exception as e:
        logger.error(f"Facial analysis assessment error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Assessment failed: {str(e)}")

class CompleteAssessmentRequest(BaseModel):
    """Request model for complete assessment"""
    session_id: str

@app.post("/api/assessment/complete")
async def complete_assessment(request: CompleteAssessmentRequest):
    """Generate final assessment combining all stages"""
    try:
        session_id = request.session_id
        
        # For demo purposes, create a mock final result since we're having DB serialization issues
        # In production, this would properly retrieve and combine all stage results
        
        # Get the latest results from each stage for demo
        try:
            behavioral = await db.assessments.find_one({'stage': 'behavioral'}, sort=[('timestamp', -1)])
            eye_tracking = await db.assessments.find_one({'stage': 'eye_tracking'}, sort=[('timestamp', -1)])
            facial = await db.assessments.find_one({'stage': 'facial_analysis'}, sort=[('timestamp', -1)])
        except Exception as db_error:
            logger.warning(f"Database retrieval error: {db_error}")
            behavioral = eye_tracking = facial = None
        
        # Create mock stage results for demo
        stage_results = {}
        if behavioral:
            stage_results['behavioral'] = {
                'prediction': behavioral['result']['prediction'],
                'probability': behavioral['result']['probability'],
                'confidence': behavioral['result']['confidence']
            }
        if eye_tracking:
            stage_results['eye_tracking'] = {
                'prediction': eye_tracking['result']['prediction'],
                'probability': eye_tracking['result']['probability'],
                'confidence': eye_tracking['result']['confidence']
            }
        if facial:
            stage_results['facial_analysis'] = {
                'prediction': facial['result']['prediction'],
                'probability': facial['result']['probability'],
                'confidence': facial['result']['confidence']
            }
        
        # Calculate final prediction with weights
        stage_weights = {'behavioral': 0.6, 'eye_tracking': 0.25, 'facial_analysis': 0.15}
        
        weighted_score = 0
        total_weight = 0
        
        for stage, result in stage_results.items():
            if stage in stage_weights:
                prob = result['probability']
                weight = stage_weights[stage]
                weighted_score += prob * weight
                total_weight += weight
        
        # Final prediction
        if total_weight > 0:
            final_probability = weighted_score / total_weight
        else:
            final_probability = 0.5
            
        final_prediction = 1 if final_probability > 0.5 else 0
        confidence = abs(final_probability - 0.5) * 2
        
        # Generate simple explanation
        explanation = {
            'overall_result': f"Multi-stage assessment {'indicates ASD' if final_prediction else 'does not indicate ASD'} with {'high' if confidence > 0.7 else 'moderate'} confidence",
            'stage_contributions': {},
            'clinical_recommendations': [
                "Recommend comprehensive diagnostic evaluation by autism specialist" if final_prediction else "Continue monitoring developmental milestones",
                "Consider additional standardized assessments" if final_prediction else "Discuss results with healthcare provider",
                "Connect with local autism support resources" if final_prediction else "Stay informed about autism awareness"
            ]
        }
        
        # Add stage contributions
        for stage, result in stage_results.items():
            explanation['stage_contributions'][stage] = {
                'prediction': 'ASD indicated' if result['prediction'] else 'ASD not indicated',
                'confidence': result['confidence'],
                'contribution': 'supporting' if result['prediction'] == final_prediction else 'conflicting'
            }
        
        final_result = {
            'session_id': session_id,
            'final_prediction': int(final_prediction),
            'final_probability': float(final_probability),
            'confidence_score': float(confidence),
            'stage_results': stage_results,
            'explanation': explanation,
            'assessment_date': datetime.now().isoformat(),
            'stages_completed': len(stage_results)
        }
        
        return final_result
        
    except Exception as e:
        logger.error(f"Complete assessment error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Assessment failed: {str(e)}")

def generate_behavioral_explanation(prediction, probability, top_features):
    """Generate explanation for behavioral assessment"""
    result_text = "indicates ASD patterns" if prediction else "does not indicate ASD patterns"
    confidence_text = "high" if probability > 0.8 or probability < 0.2 else "moderate"
    
    explanation = {
        'summary': f"Behavioral assessment {result_text} with {confidence_text} confidence",
        'key_indicators': [],
        'feature_analysis': top_features,
        'recommendations': []
    }
    
    # Analyze key features
    if 'A6_Score' in top_features and top_features['A6_Score']['value'] == 1:
        explanation['key_indicators'].append("Sensory sensitivity patterns detected")
    
    if 'A9_Score' in top_features and top_features['A9_Score']['value'] == 1:
        explanation['key_indicators'].append("Behavioral flexibility concerns noted")
    
    if 'A5_Score' in top_features and top_features['A5_Score']['value'] == 1:
        explanation['key_indicators'].append("Attention to detail patterns observed")
    
    # Add recommendations
    if prediction:
        explanation['recommendations'] = [
            "Consider comprehensive diagnostic evaluation",
            "Proceed with eye tracking and facial analysis",
            "Consult with autism specialist"
        ]
    else:
        explanation['recommendations'] = [
            "Continue with additional assessments for complete evaluation",
            "Monitor development patterns over time"
        ]
    
    return explanation

def generate_eye_tracking_explanation(prediction, probability, top_features):
    """Generate explanation for eye tracking assessment"""
    result_text = "suggests ASD patterns" if prediction else "does not suggest ASD patterns"
    
    explanation = {
        'summary': f"Eye tracking analysis {result_text}",
        'gaze_patterns': {},
        'feature_analysis': top_features,
        'clinical_significance': []
    }
    
    # Analyze gaze patterns
    if 'std_x' in top_features or 'std_y' in top_features:
        explanation['gaze_patterns']['spatial_variability'] = "Atypical gaze distribution patterns detected"
    
    if 'fixation_count' in top_features:
        explanation['gaze_patterns']['attention_patterns'] = "Unusual visual attention duration observed"
    
    if 'mean_saccade' in top_features:
        explanation['gaze_patterns']['eye_movements'] = "Atypical saccadic eye movement patterns"
    
    # Clinical significance
    explanation['clinical_significance'] = [
        "Eye tracking provides objective measures of visual attention",
        "Gaze patterns can indicate social attention differences",
        "Results should be interpreted alongside other assessments"
    ]
    
    return explanation

def generate_facial_explanation(prediction, score, data):
    """Generate explanation for facial analysis"""
    explanation = []
    
    if data.attention_patterns.get('attention_to_faces', 0) < 0.5:
        explanation.append("Reduced attention to facial regions detected")
    
    if len(data.emotion_scores) > 0:
        dominant_emotion = max(data.emotion_scores, key=data.emotion_scores.get)
        explanation.append(f"Dominant emotional expression: {dominant_emotion}")
    
    if not explanation:
        explanation.append("Facial analysis completed with standard patterns")
    
    return explanation

def generate_comprehensive_explanation(prediction, probability, stage_results):
    """Generate comprehensive explanation combining all stages"""
    result_text = "indicates ASD" if prediction else "does not indicate ASD"
    confidence_level = "high" if abs(probability - 0.5) > 0.3 else "moderate"
    
    explanation = {
        'overall_result': f"Multi-stage assessment {result_text} with {confidence_level} confidence",
        'stage_contributions': {},
        'key_findings': [],
        'clinical_recommendations': [],
        'next_steps': []
    }
    
    # Analyze each stage contribution
    for stage, result in stage_results.items():
        stage_pred = result.get('prediction', 0)
        stage_prob = result.get('probability', 0.5)
        
        explanation['stage_contributions'][stage] = {
            'prediction': 'ASD indicated' if stage_pred else 'ASD not indicated',
            'confidence': abs(stage_prob - 0.5) * 2,
            'contribution': 'supporting' if stage_pred == prediction else 'conflicting'
        }
    
    # Key findings
    if 'behavioral' in stage_results and stage_results['behavioral'].get('prediction'):
        explanation['key_findings'].append("Behavioral questionnaire indicates ASD patterns")
    
    if 'eye_tracking' in stage_results and stage_results['eye_tracking'].get('prediction'):
        explanation['key_findings'].append("Eye tracking reveals atypical gaze patterns")
    
    if 'facial_analysis' in stage_results and stage_results['facial_analysis'].get('prediction'):
        explanation['key_findings'].append("Facial analysis suggests social attention differences")
    
    # Clinical recommendations
    if prediction:
        explanation['clinical_recommendations'] = [
            "Recommend comprehensive diagnostic evaluation by autism specialist",
            "Consider additional standardized assessments (ADOS, ADI-R)",
            "Evaluate for co-occurring conditions",
            "Discuss early intervention options"
        ]
        explanation['next_steps'] = [
            "Schedule appointment with developmental pediatrician",
            "Begin documentation of behaviors and development",
            "Connect with local autism support resources"
        ]
    else:
        explanation['clinical_recommendations'] = [
            "Continue monitoring developmental milestones",
            "Consider re-evaluation if concerns persist",
            "Discuss results with healthcare provider"
        ]
        explanation['next_steps'] = [
            "Maintain regular developmental check-ups",
            "Address any specific behavioral concerns",
            "Stay informed about autism awareness"
        ]
    
    return explanation

def get_eye_tracking_description(feature_name):
    """Get description for eye tracking features"""
    descriptions = {
        'fixation_count': 'Number of visual fixations - indicates attention patterns',
        'mean_saccade': 'Average saccadic eye movement - relates to visual scanning',
        'max_saccade': 'Maximum saccadic movement - indicates eye movement range',
        'std_saccade': 'Saccadic movement variability - shows consistency patterns',
        'mean_x': 'Average horizontal gaze position - indicates gaze centering',
        'mean_y': 'Average vertical gaze position - indicates gaze height preference',
        'std_x': 'Horizontal gaze variability - shows scanning patterns',
        'std_y': 'Vertical gaze variability - indicates vertical attention spread',
        'mean_pupil': 'Average pupil diameter - relates to arousal and attention'
    }
    return descriptions.get(feature_name, 'Eye tracking measurement')

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"🚀 Starting ASD Detection Server...")
    logger.info(f"📱 Port: {port}")
    logger.info(f"🔧 API Docs: http://0.0.0.0:{port}/api/docs")
    logger.info(f"❤️  Health: http://0.0.0.0:{port}/health")
    logger.info("=" * 50)
    
    # Start server immediately - models will load on first request
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")